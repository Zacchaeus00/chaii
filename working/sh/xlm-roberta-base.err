DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:35<00:35, 35.03s/ba]100%|██████████| 2/2 [00:35<00:00, 14.59s/ba]100%|██████████| 2/2 [00:35<00:00, 17.65s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.37s/ba]100%|██████████| 1/1 [00:04<00:00,  4.37s/ba]
Using amp fp16 backend
***** Running training *****
  Num examples = 13270
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 1245
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-100] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-300] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-800] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-900] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1000] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1471
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-1100] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-base-squad2/fold0/checkpoint-700 (score: 0.2365346997976303).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:06<00:00,  6.02s/ba]100%|██████████| 1/1 [00:06<00:00,  6.02s/ba]
  0%|          | 0/1471 [00:00<?, ?ex/s]  7%|▋         | 99/1471 [00:00<00:01, 983.72ex/s] 13%|█▎        | 198/1471 [00:00<00:01, 979.71ex/s] 20%|██        | 297/1471 [00:00<00:01, 983.38ex/s] 27%|██▋       | 396/1471 [00:00<00:01, 980.97ex/s] 34%|███▎      | 495/1471 [00:00<00:01, 975.81ex/s] 40%|████      | 594/1471 [00:00<00:00, 980.55ex/s] 47%|████▋     | 693/1471 [00:00<00:00, 979.69ex/s] 54%|█████▍    | 792/1471 [00:00<00:00, 981.50ex/s] 61%|██████    | 892/1471 [00:00<00:00, 986.50ex/s] 67%|██████▋   | 991/1471 [00:01<00:00, 986.50ex/s] 74%|███████▍  | 1090/1471 [00:01<00:00, 821.32ex/s] 81%|████████  | 1189/1471 [00:01<00:00, 864.08ex/s] 88%|████████▊ | 1289/1471 [00:01<00:00, 899.93ex/s] 94%|█████████▍| 1388/1471 [00:01<00:00, 925.15ex/s]100%|██████████| 1471/1471 [00:01<00:00, 943.56ex/s]
***** Running Prediction *****
  Num examples = 1471
  Batch size = 32
  0%|          | 0/112 [00:00<?, ?it/s]  4%|▍         | 5/112 [00:00<00:02, 39.71it/s]  8%|▊         | 9/112 [00:00<00:02, 34.95it/s] 12%|█▏        | 13/112 [00:00<00:03, 28.26it/s] 16%|█▌        | 18/112 [00:00<00:02, 33.57it/s] 21%|██        | 23/112 [00:00<00:02, 38.41it/s] 25%|██▌       | 28/112 [00:00<00:02, 36.17it/s] 29%|██▊       | 32/112 [00:01<00:03, 21.88it/s] 36%|███▌      | 40/112 [00:01<00:02, 32.42it/s] 40%|████      | 45/112 [00:01<00:01, 36.02it/s] 45%|████▍     | 50/112 [00:01<00:02, 29.91it/s] 49%|████▉     | 55/112 [00:01<00:01, 29.91it/s] 56%|█████▋    | 63/112 [00:01<00:01, 39.74it/s] 62%|██████▏   | 69/112 [00:01<00:00, 43.92it/s] 67%|██████▋   | 75/112 [00:02<00:00, 40.57it/s] 71%|███████▏  | 80/112 [00:02<00:00, 39.29it/s] 76%|███████▌  | 85/112 [00:02<00:00, 32.44it/s] 80%|████████  | 90/112 [00:02<00:00, 32.62it/s] 85%|████████▍ | 95/112 [00:02<00:00, 35.60it/s] 91%|█████████ | 102/112 [00:02<00:00, 38.47it/s] 96%|█████████▌| 107/112 [00:03<00:00, 39.13it/s]100%|██████████| 112/112 [00:03<00:00, 35.69it/s]100%|██████████| 112/112 [00:03<00:00, 34.81it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:35<00:35, 35.67s/ba]100%|██████████| 2/2 [00:35<00:00, 14.88s/ba]100%|██████████| 2/2 [00:35<00:00, 18.00s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.42s/ba]100%|██████████| 1/1 [00:03<00:00,  3.42s/ba]
loading configuration file ../../input/deepset-xlm-roberta-base-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-base-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-base-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-base-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 13317
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 1251
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-300] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-700] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-800] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-900] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1000] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1424
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-1100] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-base-squad2/fold1/checkpoint-400 (score: 0.2597341537475586).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.14s/ba]100%|██████████| 1/1 [00:05<00:00,  5.14s/ba]
  0%|          | 0/1424 [00:00<?, ?ex/s]  7%|▋         | 100/1424 [00:00<00:01, 995.17ex/s] 14%|█▍        | 200/1424 [00:00<00:01, 995.38ex/s] 21%|██        | 300/1424 [00:00<00:01, 990.18ex/s] 28%|██▊       | 400/1424 [00:00<00:01, 988.60ex/s] 35%|███▌      | 499/1424 [00:00<00:00, 989.03ex/s] 42%|████▏     | 600/1424 [00:00<00:00, 994.24ex/s] 49%|████▉     | 702/1424 [00:00<00:00, 999.51ex/s] 56%|█████▋    | 802/1424 [00:00<00:00, 993.39ex/s] 63%|██████▎   | 902/1424 [00:00<00:00, 994.35ex/s] 70%|███████   | 1002/1424 [00:01<00:00, 900.95ex/s] 77%|███████▋  | 1103/1424 [00:01<00:00, 930.50ex/s] 85%|████████▍ | 1204/1424 [00:01<00:00, 951.10ex/s] 92%|█████████▏| 1304/1424 [00:01<00:00, 963.52ex/s] 99%|█████████▊| 1404/1424 [00:01<00:00, 973.51ex/s]100%|██████████| 1424/1424 [00:01<00:00, 972.39ex/s]
***** Running Prediction *****
  Num examples = 1424
  Batch size = 32
  0%|          | 0/112 [00:00<?, ?it/s]  4%|▍         | 5/112 [00:00<00:02, 44.08it/s] 10%|▉         | 11/112 [00:00<00:02, 48.93it/s] 14%|█▍        | 16/112 [00:00<00:02, 40.07it/s] 19%|█▉        | 21/112 [00:00<00:02, 31.94it/s] 23%|██▎       | 26/112 [00:00<00:02, 30.43it/s] 27%|██▋       | 30/112 [00:01<00:03, 24.43it/s] 30%|███       | 34/112 [00:01<00:02, 26.82it/s] 34%|███▍      | 38/112 [00:01<00:02, 28.99it/s] 38%|███▊      | 43/112 [00:01<00:02, 33.51it/s] 44%|████▍     | 49/112 [00:01<00:01, 39.38it/s] 48%|████▊     | 54/112 [00:01<00:01, 34.85it/s] 52%|█████▏    | 58/112 [00:01<00:01, 32.39it/s] 55%|█████▌    | 62/112 [00:01<00:01, 33.22it/s] 62%|██████▏   | 69/112 [00:02<00:01, 40.20it/s] 67%|██████▋   | 75/112 [00:02<00:00, 42.79it/s] 71%|███████▏  | 80/112 [00:02<00:00, 40.95it/s] 77%|███████▋  | 86/112 [00:02<00:00, 44.94it/s] 81%|████████▏ | 91/112 [00:02<00:00, 40.27it/s] 86%|████████▌ | 96/112 [00:02<00:00, 38.02it/s] 89%|████████▉ | 100/112 [00:02<00:00, 33.01it/s] 95%|█████████▍| 106/112 [00:02<00:00, 37.50it/s] 98%|█████████▊| 110/112 [00:03<00:00, 37.26it/s]100%|██████████| 112/112 [00:03<00:00, 36.17it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:35<00:35, 35.70s/ba]100%|██████████| 2/2 [00:36<00:00, 14.88s/ba]100%|██████████| 2/2 [00:36<00:00, 18.00s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.46s/ba]100%|██████████| 1/1 [00:03<00:00,  3.46s/ba]
loading configuration file ../../input/deepset-xlm-roberta-base-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-base-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-base-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-base-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 13332
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 1251
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-100] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-400] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-800] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-900] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1000] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1409
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-1100] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-base-squad2/fold2/checkpoint-700 (score: 0.2931174039840698).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.12s/ba]100%|██████████| 1/1 [00:05<00:00,  5.12s/ba]
  0%|          | 0/1409 [00:00<?, ?ex/s]  7%|▋         | 99/1409 [00:00<00:01, 982.25ex/s] 14%|█▍        | 199/1409 [00:00<00:01, 987.35ex/s] 21%|██        | 298/1409 [00:00<00:01, 987.61ex/s] 28%|██▊       | 397/1409 [00:00<00:01, 984.82ex/s] 35%|███▌      | 496/1409 [00:00<00:00, 979.32ex/s] 42%|████▏     | 596/1409 [00:00<00:00, 983.67ex/s] 49%|████▉     | 696/1409 [00:00<00:00, 986.89ex/s] 56%|█████▋    | 795/1409 [00:00<00:00, 985.47ex/s] 64%|██████▎   | 896/1409 [00:00<00:00, 993.00ex/s] 71%|███████   | 996/1409 [00:01<00:00, 992.72ex/s] 78%|███████▊  | 1096/1409 [00:01<00:00, 897.92ex/s] 85%|████████▍ | 1196/1409 [00:01<00:00, 926.14ex/s] 92%|█████████▏| 1294/1409 [00:01<00:00, 941.08ex/s] 99%|█████████▉| 1394/1409 [00:01<00:00, 956.64ex/s]100%|██████████| 1409/1409 [00:01<00:00, 965.10ex/s]
***** Running Prediction *****
  Num examples = 1409
  Batch size = 32
  0%|          | 0/112 [00:00<?, ?it/s]  2%|▏         | 2/112 [00:00<00:07, 14.28it/s]  4%|▍         | 5/112 [00:00<00:05, 20.74it/s]  7%|▋         | 8/112 [00:00<00:04, 21.70it/s] 12%|█▎        | 14/112 [00:00<00:03, 31.53it/s] 16%|█▌        | 18/112 [00:00<00:03, 29.09it/s] 19%|█▉        | 21/112 [00:00<00:03, 26.70it/s] 22%|██▏       | 25/112 [00:00<00:03, 28.28it/s] 25%|██▌       | 28/112 [00:01<00:03, 22.59it/s] 29%|██▊       | 32/112 [00:01<00:03, 26.21it/s] 32%|███▏      | 36/112 [00:01<00:03, 24.84it/s] 39%|███▉      | 44/112 [00:01<00:01, 35.36it/s] 46%|████▌     | 51/112 [00:01<00:01, 37.34it/s] 50%|█████     | 56/112 [00:01<00:01, 39.96it/s] 54%|█████▍    | 61/112 [00:01<00:01, 41.53it/s] 62%|██████▏   | 69/112 [00:02<00:00, 48.48it/s] 66%|██████▌   | 74/112 [00:02<00:01, 36.01it/s] 73%|███████▎  | 82/112 [00:02<00:00, 45.14it/s] 79%|███████▊  | 88/112 [00:02<00:00, 47.60it/s] 84%|████████▍ | 94/112 [00:02<00:00, 50.25it/s] 89%|████████▉ | 100/112 [00:02<00:00, 39.27it/s] 94%|█████████▍| 105/112 [00:02<00:00, 41.43it/s] 98%|█████████▊| 110/112 [00:03<00:00, 40.91it/s]100%|██████████| 112/112 [00:03<00:00, 36.21it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.75s/ba]100%|██████████| 2/2 [00:35<00:00, 14.48s/ba]100%|██████████| 2/2 [00:35<00:00, 17.53s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.32s/ba]100%|██████████| 1/1 [00:04<00:00,  4.32s/ba]
loading configuration file ../../input/deepset-xlm-roberta-base-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-base-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-base-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-base-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 13158
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 1236
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-100] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1583
  Batch size = 32
Saving model checkpoint to ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-base-squad2/fold3/checkpoint-500] due to args.save_total_limit
