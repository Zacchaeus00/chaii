DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.25s/ba]100%|██████████| 2/2 [00:34<00:00, 14.26s/ba]100%|██████████| 2/2 [00:34<00:00, 17.26s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.33s/ba]100%|██████████| 1/1 [00:04<00:00,  4.33s/ba]
Using amp fp16 backend
***** Running training *****
  Num examples = 8855
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 828
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-300] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 978
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold0/checkpoint-500 (score: 0.22646868228912354).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.91s/ba]100%|██████████| 1/1 [00:05<00:00,  5.91s/ba]
  0%|          | 0/978 [00:00<?, ?ex/s]  8%|▊         | 78/978 [00:00<00:01, 779.50ex/s] 16%|█▌        | 157/978 [00:00<00:01, 781.59ex/s] 24%|██▍       | 237/978 [00:00<00:00, 785.25ex/s] 32%|███▏      | 316/978 [00:00<00:00, 780.67ex/s] 40%|████      | 396/978 [00:00<00:00, 784.25ex/s] 49%|████▊     | 475/978 [00:00<00:00, 782.79ex/s] 57%|█████▋    | 555/978 [00:00<00:00, 785.82ex/s] 65%|██████▍   | 635/978 [00:00<00:00, 789.73ex/s] 73%|███████▎  | 714/978 [00:00<00:00, 789.78ex/s] 81%|████████  | 793/978 [00:01<00:00, 787.13ex/s] 89%|████████▉ | 873/978 [00:01<00:00, 789.02ex/s] 97%|█████████▋| 952/978 [00:01<00:00, 787.79ex/s]100%|██████████| 978/978 [00:01<00:00, 782.57ex/s]
***** Running Prediction *****
  Num examples = 978
  Batch size = 8
  0%|          | 0/112 [00:00<?, ?it/s]  4%|▍         | 5/112 [00:00<00:02, 47.58it/s]  9%|▉         | 10/112 [00:00<00:02, 42.16it/s] 13%|█▎        | 15/112 [00:00<00:02, 34.64it/s] 20%|█▉        | 22/112 [00:00<00:02, 44.82it/s] 24%|██▍       | 27/112 [00:00<00:01, 43.24it/s] 29%|██▊       | 32/112 [00:00<00:02, 27.10it/s] 37%|███▋      | 41/112 [00:01<00:01, 37.06it/s] 42%|████▏     | 47/112 [00:01<00:01, 40.07it/s] 46%|████▋     | 52/112 [00:01<00:01, 40.18it/s] 51%|█████     | 57/112 [00:01<00:01, 38.98it/s] 59%|█████▉    | 66/112 [00:01<00:00, 50.39it/s] 64%|██████▍   | 72/112 [00:01<00:00, 47.15it/s] 70%|██████▉   | 78/112 [00:01<00:00, 45.89it/s] 75%|███████▌  | 84/112 [00:02<00:00, 46.18it/s] 79%|███████▉  | 89/112 [00:02<00:00, 40.55it/s] 84%|████████▍ | 94/112 [00:02<00:00, 42.30it/s] 91%|█████████ | 102/112 [00:02<00:00, 43.43it/s] 96%|█████████▌| 107/112 [00:02<00:00, 44.71it/s]100%|██████████| 112/112 [00:02<00:00, 41.72it/s]100%|██████████| 112/112 [00:02<00:00, 41.51it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:35<00:35, 35.21s/ba]100%|██████████| 2/2 [00:35<00:00, 14.65s/ba]100%|██████████| 2/2 [00:35<00:00, 17.74s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.43s/ba]100%|██████████| 1/1 [00:03<00:00,  3.43s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8882
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 831
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 951
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold1/checkpoint-200 (score: 0.31823545694351196).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]
  0%|          | 0/951 [00:00<?, ?ex/s]  8%|▊         | 79/951 [00:00<00:01, 789.14ex/s] 17%|█▋        | 158/951 [00:00<00:01, 781.08ex/s] 25%|██▍       | 237/951 [00:00<00:00, 781.14ex/s] 33%|███▎      | 316/951 [00:00<00:00, 778.38ex/s] 42%|████▏     | 395/951 [00:00<00:00, 781.25ex/s] 50%|████▉     | 474/951 [00:00<00:00, 783.49ex/s] 58%|█████▊    | 553/951 [00:00<00:00, 782.90ex/s] 66%|██████▋   | 632/951 [00:00<00:00, 784.60ex/s] 75%|███████▍  | 712/951 [00:00<00:00, 787.06ex/s] 83%|████████▎ | 791/951 [00:01<00:00, 786.52ex/s] 91%|█████████▏| 870/951 [00:01<00:00, 784.90ex/s]100%|█████████▉| 950/951 [00:01<00:00, 787.15ex/s]100%|██████████| 951/951 [00:01<00:00, 784.58ex/s]
***** Running Prediction *****
  Num examples = 951
  Batch size = 8
  0%|          | 0/112 [00:00<?, ?it/s]  5%|▌         | 6/112 [00:00<00:01, 53.98it/s] 11%|█         | 12/112 [00:00<00:02, 41.27it/s] 17%|█▋        | 19/112 [00:00<00:02, 46.35it/s] 21%|██▏       | 24/112 [00:00<00:02, 41.08it/s] 26%|██▌       | 29/112 [00:00<00:02, 28.74it/s] 30%|███       | 34/112 [00:00<00:02, 32.57it/s] 36%|███▌      | 40/112 [00:01<00:02, 35.92it/s] 42%|████▏     | 47/112 [00:01<00:01, 43.66it/s] 47%|████▋     | 53/112 [00:01<00:01, 42.68it/s] 52%|█████▏    | 58/112 [00:01<00:01, 38.82it/s] 56%|█████▋    | 63/112 [00:01<00:01, 39.23it/s] 63%|██████▎   | 71/112 [00:01<00:00, 48.33it/s] 71%|███████   | 79/112 [00:01<00:00, 56.29it/s] 77%|███████▋  | 86/112 [00:02<00:00, 50.21it/s] 82%|████████▏ | 92/112 [00:02<00:00, 46.98it/s] 88%|████████▊ | 98/112 [00:02<00:00, 43.54it/s] 92%|█████████▏| 103/112 [00:02<00:00, 41.42it/s] 97%|█████████▋| 109/112 [00:02<00:00, 42.81it/s]100%|██████████| 112/112 [00:02<00:00, 42.85it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.86s/ba]100%|██████████| 2/2 [00:35<00:00, 14.51s/ba]100%|██████████| 2/2 [00:35<00:00, 17.56s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.41s/ba]100%|██████████| 1/1 [00:03<00:00,  3.41s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8890
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 834
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-300] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 943
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold2/checkpoint-500 (score: 0.2959666848182678).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.87s/ba]100%|██████████| 1/1 [00:04<00:00,  4.87s/ba]
  0%|          | 0/943 [00:00<?, ?ex/s]  8%|▊         | 78/943 [00:00<00:01, 771.99ex/s] 17%|█▋        | 157/943 [00:00<00:01, 779.75ex/s] 25%|██▌       | 236/943 [00:00<00:00, 778.58ex/s] 33%|███▎      | 315/943 [00:00<00:00, 781.56ex/s] 42%|████▏     | 394/943 [00:00<00:00, 781.14ex/s] 50%|█████     | 474/943 [00:00<00:00, 787.01ex/s] 59%|█████▊    | 553/943 [00:00<00:00, 780.77ex/s] 67%|██████▋   | 633/943 [00:00<00:00, 785.81ex/s] 76%|███████▌  | 712/943 [00:00<00:00, 786.24ex/s] 84%|████████▍ | 793/943 [00:01<00:00, 791.93ex/s] 93%|█████████▎| 873/943 [00:01<00:00, 787.98ex/s]100%|██████████| 943/943 [00:01<00:00, 786.57ex/s]
***** Running Prediction *****
  Num examples = 943
  Batch size = 8
  0%|          | 0/112 [00:00<?, ?it/s]  2%|▏         | 2/112 [00:00<00:06, 16.89it/s]  5%|▌         | 6/112 [00:00<00:03, 27.11it/s]  8%|▊         | 9/112 [00:00<00:03, 28.02it/s] 13%|█▎        | 15/112 [00:00<00:02, 35.42it/s] 17%|█▋        | 19/112 [00:00<00:02, 36.53it/s] 21%|██        | 23/112 [00:00<00:02, 32.58it/s] 24%|██▍       | 27/112 [00:00<00:02, 29.58it/s] 28%|██▊       | 31/112 [00:01<00:02, 28.93it/s] 31%|███▏      | 35/112 [00:01<00:02, 28.44it/s] 34%|███▍      | 38/112 [00:01<00:02, 28.64it/s] 43%|████▎     | 48/112 [00:01<00:01, 45.46it/s] 47%|████▋     | 53/112 [00:01<00:01, 40.16it/s] 54%|█████▎    | 60/112 [00:01<00:01, 45.43it/s] 62%|██████▏   | 69/112 [00:01<00:00, 55.47it/s] 67%|██████▋   | 75/112 [00:01<00:00, 44.80it/s] 75%|███████▌  | 84/112 [00:02<00:00, 53.85it/s] 80%|████████  | 90/112 [00:02<00:00, 52.84it/s] 88%|████████▊ | 98/112 [00:02<00:00, 55.78it/s] 93%|█████████▎| 104/112 [00:02<00:00, 47.90it/s] 98%|█████████▊| 110/112 [00:02<00:00, 47.79it/s]100%|██████████| 112/112 [00:02<00:00, 42.07it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.38s/ba]100%|██████████| 2/2 [00:34<00:00, 14.32s/ba]100%|██████████| 2/2 [00:34<00:00, 17.33s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.30s/ba]100%|██████████| 1/1 [00:04<00:00,  4.30s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8781
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 822
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-500] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1052
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold3/checkpoint-300 (score: 0.2546149790287018).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.96s/ba]100%|██████████| 1/1 [00:05<00:00,  5.96s/ba]
  0%|          | 0/1052 [00:00<?, ?ex/s]  7%|▋         | 78/1052 [00:00<00:01, 777.04ex/s] 15%|█▍        | 157/1052 [00:00<00:01, 778.96ex/s] 22%|██▏       | 236/1052 [00:00<00:01, 783.65ex/s] 30%|███       | 316/1052 [00:00<00:00, 787.44ex/s] 38%|███▊      | 396/1052 [00:00<00:00, 788.79ex/s] 45%|████▌     | 475/1052 [00:00<00:00, 784.16ex/s] 53%|█████▎    | 554/1052 [00:00<00:00, 785.26ex/s] 60%|██████    | 633/1052 [00:00<00:00, 776.58ex/s] 68%|██████▊   | 712/1052 [00:00<00:00, 779.22ex/s] 75%|███████▌  | 791/1052 [00:01<00:00, 779.79ex/s] 83%|████████▎ | 871/1052 [00:01<00:00, 784.86ex/s] 90%|█████████ | 951/1052 [00:01<00:00, 788.61ex/s] 98%|█████████▊| 1030/1052 [00:01<00:00, 680.59ex/s]100%|██████████| 1052/1052 [00:01<00:00, 755.10ex/s]
***** Running Prediction *****
  Num examples = 1052
  Batch size = 8
  0%|          | 0/112 [00:00<?, ?it/s]  2%|▏         | 2/112 [00:00<00:05, 19.65it/s]  5%|▌         | 6/112 [00:00<00:03, 27.96it/s]  9%|▉         | 10/112 [00:00<00:03, 25.79it/s] 12%|█▎        | 14/112 [00:00<00:03, 25.53it/s] 18%|█▊        | 20/112 [00:00<00:02, 33.19it/s] 23%|██▎       | 26/112 [00:00<00:02, 36.59it/s] 28%|██▊       | 31/112 [00:00<00:02, 36.96it/s] 35%|███▍      | 39/112 [00:01<00:01, 47.83it/s] 40%|████      | 45/112 [00:01<00:01, 39.76it/s] 45%|████▍     | 50/112 [00:01<00:01, 31.33it/s] 52%|█████▏    | 58/112 [00:01<00:01, 35.19it/s] 57%|█████▋    | 64/112 [00:01<00:01, 37.03it/s] 62%|██████▎   | 70/112 [00:01<00:01, 40.87it/s] 67%|██████▋   | 75/112 [00:02<00:00, 40.04it/s] 71%|███████▏  | 80/112 [00:02<00:00, 39.50it/s] 76%|███████▌  | 85/112 [00:02<00:00, 41.88it/s] 80%|████████  | 90/112 [00:02<00:00, 43.10it/s] 89%|████████▉ | 100/112 [00:02<00:00, 57.35it/s] 96%|█████████▌| 107/112 [00:02<00:00, 45.70it/s]100%|██████████| 112/112 [00:02<00:00, 38.27it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.94s/ba]100%|██████████| 2/2 [00:35<00:00, 14.55s/ba]100%|██████████| 2/2 [00:35<00:00, 17.61s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.42s/ba]100%|██████████| 1/1 [00:03<00:00,  3.42s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8944
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 837
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 889
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold4/checkpoint-300 (score: 0.2803599536418915).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.84s/ba]100%|██████████| 1/1 [00:04<00:00,  4.84s/ba]
  0%|          | 0/889 [00:00<?, ?ex/s]  9%|▉         | 79/889 [00:00<00:01, 786.87ex/s] 18%|█▊        | 158/889 [00:00<00:00, 786.15ex/s] 27%|██▋       | 237/889 [00:00<00:00, 783.30ex/s] 36%|███▌      | 316/889 [00:00<00:00, 784.32ex/s] 45%|████▍     | 396/889 [00:00<00:00, 788.07ex/s] 53%|█████▎    | 475/889 [00:00<00:00, 783.20ex/s] 62%|██████▏   | 555/889 [00:00<00:00, 786.94ex/s] 71%|███████▏  | 635/889 [00:00<00:00, 789.10ex/s] 80%|████████  | 714/889 [00:00<00:00, 786.34ex/s] 89%|████████▉ | 795/889 [00:01<00:00, 791.00ex/s] 98%|█████████▊| 875/889 [00:01<00:00, 789.50ex/s]100%|██████████| 889/889 [00:01<00:00, 787.51ex/s]
***** Running Prediction *****
  Num examples = 889
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  6%|▋         | 7/111 [00:00<00:01, 69.16it/s] 13%|█▎        | 14/111 [00:00<00:01, 53.83it/s] 18%|█▊        | 20/111 [00:00<00:02, 44.67it/s] 23%|██▎       | 26/111 [00:00<00:02, 41.27it/s] 28%|██▊       | 31/111 [00:00<00:01, 42.15it/s] 32%|███▏      | 36/111 [00:00<00:01, 41.71it/s] 37%|███▋      | 41/111 [00:00<00:01, 39.08it/s] 42%|████▏     | 47/111 [00:01<00:01, 43.88it/s] 47%|████▋     | 52/111 [00:01<00:01, 40.08it/s] 51%|█████▏    | 57/111 [00:01<00:01, 40.74it/s] 57%|█████▋    | 63/111 [00:01<00:01, 44.64it/s] 61%|██████▏   | 68/111 [00:01<00:01, 41.15it/s] 66%|██████▌   | 73/111 [00:01<00:00, 41.47it/s] 72%|███████▏  | 80/111 [00:01<00:00, 44.84it/s] 77%|███████▋  | 85/111 [00:02<00:00, 36.81it/s] 87%|████████▋ | 97/111 [00:02<00:00, 53.52it/s] 93%|█████████▎| 103/111 [00:02<00:00, 48.79it/s] 98%|█████████▊| 109/111 [00:02<00:00, 50.54it/s]100%|██████████| 111/111 [00:02<00:00, 45.59it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.57s/ba]100%|██████████| 2/2 [00:34<00:00, 14.39s/ba]100%|██████████| 2/2 [00:34<00:00, 17.42s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:03<00:00,  3.49s/ba]100%|██████████| 1/1 [00:03<00:00,  3.49s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8852
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 828
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-100] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-200] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 981
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold5/checkpoint-500 (score: 0.30119144916534424).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.03s/ba]100%|██████████| 1/1 [00:05<00:00,  5.03s/ba]
  0%|          | 0/981 [00:00<?, ?ex/s]  8%|▊         | 79/981 [00:00<00:01, 785.66ex/s] 16%|█▌        | 158/981 [00:00<00:01, 785.62ex/s] 24%|██▍       | 237/981 [00:00<00:00, 781.47ex/s] 32%|███▏      | 316/981 [00:00<00:00, 783.07ex/s] 40%|████      | 395/981 [00:00<00:00, 780.72ex/s] 48%|████▊     | 475/981 [00:00<00:00, 783.96ex/s] 57%|█████▋    | 555/981 [00:00<00:00, 787.93ex/s] 65%|██████▍   | 634/981 [00:00<00:00, 786.63ex/s] 73%|███████▎  | 713/981 [00:00<00:00, 784.58ex/s] 81%|████████  | 792/981 [00:01<00:00, 779.45ex/s] 89%|████████▊ | 870/981 [00:01<00:00, 779.32ex/s] 97%|█████████▋| 950/981 [00:01<00:00, 783.66ex/s]100%|██████████| 981/981 [00:01<00:00, 783.01ex/s]
***** Running Prediction *****
  Num examples = 981
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  5%|▌         | 6/111 [00:00<00:02, 51.93it/s] 11%|█         | 12/111 [00:00<00:01, 52.00it/s] 16%|█▌        | 18/111 [00:00<00:02, 43.35it/s] 21%|██        | 23/111 [00:00<00:02, 39.86it/s] 25%|██▌       | 28/111 [00:00<00:02, 29.44it/s] 31%|███       | 34/111 [00:00<00:02, 34.58it/s] 34%|███▍      | 38/111 [00:01<00:02, 31.78it/s] 40%|███▉      | 44/111 [00:01<00:01, 38.15it/s] 45%|████▌     | 50/111 [00:01<00:01, 41.15it/s] 50%|████▉     | 55/111 [00:01<00:01, 41.67it/s] 55%|█████▍    | 61/111 [00:01<00:01, 44.64it/s] 59%|█████▉    | 66/111 [00:01<00:01, 44.55it/s] 64%|██████▍   | 71/111 [00:01<00:00, 42.72it/s] 68%|██████▊   | 76/111 [00:01<00:00, 40.52it/s] 73%|███████▎  | 81/111 [00:02<00:00, 40.38it/s] 77%|███████▋  | 86/111 [00:02<00:00, 30.92it/s] 85%|████████▍ | 94/111 [00:02<00:00, 39.95it/s] 91%|█████████ | 101/111 [00:02<00:00, 45.69it/s] 96%|█████████▋| 107/111 [00:02<00:00, 48.15it/s]100%|██████████| 111/111 [00:02<00:00, 40.96it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:33<00:33, 33.45s/ba]100%|██████████| 2/2 [00:33<00:00, 13.93s/ba]100%|██████████| 2/2 [00:33<00:00, 16.86s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8725
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 816
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1108
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold6/checkpoint-400 (score: 0.2789488136768341).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:06<00:00,  6.76s/ba]100%|██████████| 1/1 [00:06<00:00,  6.76s/ba]
  0%|          | 0/1108 [00:00<?, ?ex/s]  7%|▋         | 79/1108 [00:00<00:01, 788.63ex/s] 14%|█▍        | 158/1108 [00:00<00:01, 783.99ex/s] 21%|██▏       | 238/1108 [00:00<00:01, 787.74ex/s] 29%|██▊       | 317/1108 [00:00<00:01, 787.61ex/s] 36%|███▌      | 396/1108 [00:00<00:00, 785.10ex/s] 43%|████▎     | 476/1108 [00:00<00:00, 788.25ex/s] 50%|█████     | 557/1108 [00:00<00:00, 792.99ex/s] 57%|█████▋    | 637/1108 [00:00<00:00, 787.60ex/s] 65%|██████▍   | 716/1108 [00:00<00:00, 787.45ex/s] 72%|███████▏  | 795/1108 [00:01<00:00, 780.13ex/s] 79%|███████▉  | 874/1108 [00:01<00:00, 782.29ex/s] 86%|████████▌ | 953/1108 [00:01<00:00, 783.05ex/s] 93%|█████████▎| 1032/1108 [00:01<00:00, 683.33ex/s]100%|██████████| 1108/1108 [00:01<00:00, 759.33ex/s]
***** Running Prediction *****
  Num examples = 1108
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  5%|▌         | 6/111 [00:00<00:01, 53.29it/s] 11%|█         | 12/111 [00:00<00:03, 32.10it/s] 14%|█▍        | 16/111 [00:00<00:02, 34.19it/s] 21%|██        | 23/111 [00:00<00:02, 43.39it/s] 25%|██▌       | 28/111 [00:00<00:02, 39.66it/s] 30%|██▉       | 33/111 [00:00<00:02, 28.72it/s] 33%|███▎      | 37/111 [00:01<00:02, 30.07it/s] 40%|███▉      | 44/111 [00:01<00:01, 38.56it/s] 44%|████▍     | 49/111 [00:01<00:01, 39.00it/s] 55%|█████▍    | 61/111 [00:01<00:00, 52.91it/s] 60%|██████    | 67/111 [00:01<00:00, 44.80it/s] 65%|██████▍   | 72/111 [00:01<00:01, 34.07it/s] 69%|██████▉   | 77/111 [00:02<00:00, 35.42it/s] 73%|███████▎  | 81/111 [00:02<00:00, 33.58it/s] 77%|███████▋  | 85/111 [00:02<00:00, 31.78it/s] 80%|████████  | 89/111 [00:02<00:00, 33.45it/s] 84%|████████▍ | 93/111 [00:02<00:00, 30.77it/s] 87%|████████▋ | 97/111 [00:02<00:00, 31.44it/s] 92%|█████████▏| 102/111 [00:02<00:00, 31.50it/s] 97%|█████████▋| 108/111 [00:03<00:00, 36.00it/s]100%|██████████| 111/111 [00:03<00:00, 36.18it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:33<00:33, 33.92s/ba]100%|██████████| 2/2 [00:34<00:00, 14.13s/ba]100%|██████████| 2/2 [00:34<00:00, 17.10s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.41s/ba]100%|██████████| 1/1 [00:04<00:00,  4.41s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8789
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 822
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-100] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1044
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold7/checkpoint-200 (score: 0.3532505929470062).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:06<00:00,  6.08s/ba]100%|██████████| 1/1 [00:06<00:00,  6.08s/ba]
  0%|          | 0/1044 [00:00<?, ?ex/s]  8%|▊         | 79/1044 [00:00<00:01, 785.18ex/s] 15%|█▌        | 158/1044 [00:00<00:01, 780.47ex/s] 23%|██▎       | 237/1044 [00:00<00:01, 777.20ex/s] 30%|███       | 317/1044 [00:00<00:00, 782.34ex/s] 38%|███▊      | 396/1044 [00:00<00:00, 781.02ex/s] 46%|████▌     | 477/1044 [00:00<00:00, 787.52ex/s] 53%|█████▎    | 556/1044 [00:00<00:00, 786.20ex/s] 61%|██████    | 635/1044 [00:00<00:00, 774.88ex/s] 68%|██████▊   | 713/1044 [00:00<00:00, 772.33ex/s] 76%|███████▌  | 793/1044 [00:01<00:00, 779.10ex/s] 84%|████████▎ | 872/1044 [00:01<00:00, 779.89ex/s] 91%|█████████ | 951/1044 [00:01<00:00, 778.83ex/s] 99%|█████████▊| 1029/1044 [00:01<00:00, 680.68ex/s]100%|██████████| 1044/1044 [00:01<00:00, 753.06ex/s]
***** Running Prediction *****
  Num examples = 1044
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  4%|▎         | 4/111 [00:00<00:02, 39.81it/s]  9%|▉         | 10/111 [00:00<00:02, 49.86it/s] 14%|█▎        | 15/111 [00:00<00:02, 43.08it/s] 18%|█▊        | 20/111 [00:00<00:02, 39.61it/s] 23%|██▎       | 25/111 [00:00<00:02, 31.69it/s] 27%|██▋       | 30/111 [00:00<00:02, 34.09it/s] 32%|███▏      | 36/111 [00:00<00:01, 37.75it/s] 36%|███▌      | 40/111 [00:01<00:02, 33.34it/s] 40%|███▉      | 44/111 [00:01<00:01, 34.85it/s] 46%|████▌     | 51/111 [00:01<00:01, 37.85it/s] 51%|█████▏    | 57/111 [00:01<00:01, 41.79it/s] 56%|█████▌    | 62/111 [00:01<00:01, 37.07it/s] 61%|██████▏   | 68/111 [00:01<00:01, 35.98it/s] 65%|██████▍   | 72/111 [00:02<00:01, 29.28it/s] 74%|███████▍  | 82/111 [00:02<00:00, 41.14it/s] 78%|███████▊  | 87/111 [00:02<00:00, 39.20it/s] 85%|████████▍ | 94/111 [00:02<00:00, 45.28it/s] 89%|████████▉ | 99/111 [00:02<00:00, 44.03it/s] 94%|█████████▎| 104/111 [00:02<00:00, 43.62it/s] 98%|█████████▊| 109/111 [00:02<00:00, 35.47it/s]100%|██████████| 111/111 [00:02<00:00, 38.31it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:34<00:34, 34.34s/ba]100%|██████████| 2/2 [00:34<00:00, 14.30s/ba]100%|██████████| 2/2 [00:34<00:00, 17.31s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.17s/ba]100%|██████████| 1/1 [00:04<00:00,  4.17s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 8747
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 819
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-100] due to args.save_total_limit
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-200] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 1086
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold8/checkpoint-400 (score: 0.2843344807624817).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:05<00:00,  5.88s/ba]100%|██████████| 1/1 [00:05<00:00,  5.88s/ba]
  0%|          | 0/1086 [00:00<?, ?ex/s]  7%|▋         | 79/1086 [00:00<00:01, 786.77ex/s] 15%|█▍        | 158/1086 [00:00<00:01, 781.44ex/s] 22%|██▏       | 237/1086 [00:00<00:01, 779.38ex/s] 29%|██▉       | 315/1086 [00:00<00:00, 777.51ex/s] 36%|███▌      | 393/1086 [00:00<00:00, 775.35ex/s] 43%|████▎     | 472/1086 [00:00<00:00, 777.56ex/s] 51%|█████     | 550/1086 [00:00<00:00, 777.05ex/s] 58%|█████▊    | 629/1086 [00:00<00:00, 780.39ex/s] 65%|██████▌   | 708/1086 [00:00<00:00, 777.62ex/s] 72%|███████▏  | 786/1086 [00:01<00:00, 776.71ex/s] 80%|███████▉  | 866/1086 [00:01<00:00, 781.06ex/s] 87%|████████▋ | 945/1086 [00:01<00:00, 766.09ex/s] 94%|█████████▍| 1022/1086 [00:01<00:00, 673.14ex/s]100%|██████████| 1086/1086 [00:01<00:00, 750.16ex/s]
***** Running Prediction *****
  Num examples = 1086
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  5%|▌         | 6/111 [00:00<00:01, 54.39it/s] 11%|█         | 12/111 [00:00<00:02, 42.00it/s] 15%|█▌        | 17/111 [00:00<00:02, 38.55it/s] 19%|█▉        | 21/111 [00:00<00:02, 33.05it/s] 23%|██▎       | 25/111 [00:00<00:02, 32.09it/s] 26%|██▌       | 29/111 [00:00<00:02, 31.91it/s] 30%|██▉       | 33/111 [00:00<00:02, 31.22it/s] 33%|███▎      | 37/111 [00:01<00:02, 29.97it/s] 37%|███▋      | 41/111 [00:01<00:02, 28.38it/s] 43%|████▎     | 48/111 [00:01<00:01, 37.50it/s] 48%|████▊     | 53/111 [00:01<00:01, 34.49it/s] 56%|█████▌    | 62/111 [00:01<00:01, 46.66it/s] 61%|██████▏   | 68/111 [00:01<00:01, 41.82it/s] 66%|██████▌   | 73/111 [00:02<00:01, 35.86it/s] 71%|███████   | 79/111 [00:02<00:00, 38.41it/s] 76%|███████▌  | 84/111 [00:02<00:00, 40.40it/s] 81%|████████  | 90/111 [00:02<00:00, 41.11it/s] 86%|████████▌ | 95/111 [00:02<00:00, 43.17it/s] 90%|█████████ | 100/111 [00:02<00:00, 41.62it/s] 96%|█████████▋| 107/111 [00:02<00:00, 47.29it/s]100%|██████████| 111/111 [00:03<00:00, 36.69it/s]
PyTorch: setting up devices
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:35<00:35, 35.57s/ba]100%|██████████| 2/2 [00:35<00:00, 14.72s/ba]100%|██████████| 2/2 [00:35<00:00, 17.85s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:02<00:00,  2.71s/ba]100%|██████████| 1/1 [00:02<00:00,  2.71s/ba]
loading configuration file ../../input/deepset-xlm-roberta-large-squad2/config.json
Model config XLMRobertaConfig {
  "_name_or_path": "deepset/xlm-roberta-large-squad2",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "language": "english",
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "name": "XLMRoberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

loading weights file ../../input/deepset-xlm-roberta-large-squad2/pytorch_model.bin
All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.

All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at ../../input/deepset-xlm-roberta-large-squad2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.
Using amp fp16 backend
***** Running training *****
  Num examples = 9032
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 846
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-100] due to args.save_total_limit
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300/special_tokens_map.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-300] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-400] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-500] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-600] due to args.save_total_limit
***** Running Evaluation *****
  Num examples = 801
  Batch size = 8
Saving model checkpoint to ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-800
Configuration saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-800/config.json
Model weights saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-800/pytorch_model.bin
tokenizer config file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-800/tokenizer_config.json
Special tokens file saved in ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-800/special_tokens_map.json
Deleting older checkpoint [../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-700] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../model/deepset-xlm-roberta-large-squad2-maxlen512-stride128/fold9/checkpoint-200 (score: 0.39709237217903137).
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:04<00:00,  4.00s/ba]100%|██████████| 1/1 [00:04<00:00,  4.00s/ba]
  0%|          | 0/801 [00:00<?, ?ex/s] 10%|▉         | 78/801 [00:00<00:00, 778.75ex/s] 20%|█▉        | 158/801 [00:00<00:00, 786.06ex/s] 30%|██▉       | 238/801 [00:00<00:00, 788.05ex/s] 40%|███▉      | 317/801 [00:00<00:00, 788.32ex/s] 49%|████▉     | 396/801 [00:00<00:00, 788.65ex/s] 59%|█████▉    | 476/801 [00:00<00:00, 789.89ex/s] 70%|██████▉   | 557/801 [00:00<00:00, 794.47ex/s] 80%|███████▉  | 637/801 [00:00<00:00, 792.79ex/s] 90%|████████▉ | 717/801 [00:00<00:00, 791.15ex/s]100%|█████████▉| 797/801 [00:01<00:00, 790.40ex/s]100%|██████████| 801/801 [00:01<00:00, 789.97ex/s]
***** Running Prediction *****
  Num examples = 801
  Batch size = 8
  0%|          | 0/111 [00:00<?, ?it/s]  4%|▎         | 4/111 [00:00<00:03, 32.53it/s]  8%|▊         | 9/111 [00:00<00:02, 41.39it/s] 13%|█▎        | 14/111 [00:00<00:02, 44.43it/s] 17%|█▋        | 19/111 [00:00<00:02, 38.69it/s] 22%|██▏       | 24/111 [00:00<00:02, 41.92it/s] 27%|██▋       | 30/111 [00:00<00:01, 41.49it/s] 32%|███▏      | 35/111 [00:00<00:01, 42.29it/s] 40%|███▉      | 44/111 [00:01<00:01, 43.19it/s] 46%|████▌     | 51/111 [00:01<00:01, 45.40it/s] 54%|█████▍    | 60/111 [00:01<00:00, 53.42it/s] 60%|██████    | 67/111 [00:01<00:00, 56.64it/s] 68%|██████▊   | 76/111 [00:01<00:00, 63.42it/s] 75%|███████▍  | 83/111 [00:01<00:00, 57.14it/s] 80%|████████  | 89/111 [00:01<00:00, 57.09it/s] 87%|████████▋ | 97/111 [00:01<00:00, 62.00it/s] 94%|█████████▎| 104/111 [00:02<00:00, 50.85it/s]100%|██████████| 111/111 [00:02<00:00, 52.83it/s]100%|██████████| 111/111 [00:02<00:00, 50.32it/s]
