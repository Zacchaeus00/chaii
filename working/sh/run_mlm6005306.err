DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 3196.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.19it/s]100%|██████████| 1/1 [00:00<00:00,  6.18it/s]
0 tables [00:00, ? tables/s]1 tables [00:01,  1.33s/ tables]2 tables [00:02,  1.04 tables/s]3 tables [00:02,  1.17 tables/s]4 tables [00:03,  1.26 tables/s]5 tables [00:04,  1.28 tables/s]6 tables [00:04,  1.33 tables/s]7 tables [00:05,  1.33 tables/s]8 tables [00:06,  1.35 tables/s]9 tables [00:07,  1.39 tables/s]10 tables [00:07,  1.40 tables/s]11 tables [00:08,  1.38 tables/s]12 tables [00:09,  1.40 tables/s]13 tables [00:09,  1.37 tables/s]14 tables [00:10,  1.38 tables/s]15 tables [00:11,  1.42 tables/s]16 tables [00:12,  1.42 tables/s]17 tables [00:12,  1.39 tables/s]18 tables [00:13,  1.43 tables/s]19 tables [00:14,  1.46 tables/s]20 tables [00:14,  1.42 tables/s]21 tables [00:15,  1.41 tables/s]22 tables [00:16,  1.40 tables/s]23 tables [00:17,  1.38 tables/s]24 tables [00:17,  1.37 tables/s]25 tables [00:18,  1.37 tables/s]26 tables [00:18,  1.60 tables/s]                                   0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.11s/it]100%|██████████| 1/1 [00:02<00:00,  2.11s/it]
[INFO|configuration_utils.py:581] 2021-09-24 00:24:24,324 >> loading configuration file ../../input/microsoft-infoxlm-large/config.json
/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
[INFO|configuration_utils.py:620] 2021-09-24 00:24:24,355 >> Model config XLMRobertaConfig {
  "_name_or_path": "microsoft/infoxlm-large",
  "architectures": [
    "XLMRobertaForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.11.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

[INFO|tokenization_utils_base.py:1671] 2021-09-24 00:24:24,471 >> Didn't find file ../../input/microsoft-infoxlm-large/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1739] 2021-09-24 00:24:24,472 >> loading file ../../input/microsoft-infoxlm-large/sentencepiece.bpe.model
[INFO|tokenization_utils_base.py:1739] 2021-09-24 00:24:24,472 >> loading file ../../input/microsoft-infoxlm-large/tokenizer.json
[INFO|tokenization_utils_base.py:1739] 2021-09-24 00:24:24,472 >> loading file None
[INFO|tokenization_utils_base.py:1739] 2021-09-24 00:24:24,472 >> loading file ../../input/microsoft-infoxlm-large/special_tokens_map.json
[INFO|tokenization_utils_base.py:1739] 2021-09-24 00:24:24,472 >> loading file ../../input/microsoft-infoxlm-large/tokenizer_config.json
[INFO|modeling_utils.py:1321] 2021-09-24 00:24:25,496 >> loading weights file ../../input/microsoft-infoxlm-large/pytorch_model.bin
[WARNING|modeling_utils.py:1579] 2021-09-24 00:24:35,715 >> Some weights of the model checkpoint at ../../input/microsoft-infoxlm-large were not used when initializing XLMRobertaForMaskedLM: ['qa_outputs.bias', 'qa_outputs.weight']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1590] 2021-09-24 00:24:35,715 >> Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../input/microsoft-infoxlm-large and are newly initialized: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on every text in dataset:   0%|          | 0/243 [00:00<?, ?ba/s]Running tokenizer on every text in dataset:   0%|          | 1/243 [00:00<03:43,  1.08ba/s]Running tokenizer on every text in dataset:   1%|          | 2/243 [00:01<03:01,  1.33ba/s]Running tokenizer on every text in dataset:   1%|          | 3/243 [00:02<02:44,  1.46ba/s]Running tokenizer on every text in dataset:   2%|▏         | 4/243 [00:02<02:44,  1.45ba/s]Running tokenizer on every text in dataset:   2%|▏         | 5/243 [00:03<02:32,  1.56ba/s]Running tokenizer on every text in dataset:   2%|▏         | 6/243 [00:03<02:21,  1.68ba/s]Running tokenizer on every text in dataset:   3%|▎         | 7/243 [00:04<02:15,  1.74ba/s]Running tokenizer on every text in dataset:   3%|▎         | 8/243 [00:05<02:34,  1.52ba/s]Running tokenizer on every text in dataset:   4%|▎         | 9/243 [00:05<02:35,  1.51ba/s]Running tokenizer on every text in dataset:   4%|▍         | 10/243 [00:06<02:23,  1.62ba/s]Running tokenizer on every text in dataset:   5%|▍         | 11/243 [00:07<02:20,  1.65ba/s]Running tokenizer on every text in dataset:   5%|▍         | 12/243 [00:07<02:18,  1.66ba/s]Running tokenizer on every text in dataset:   5%|▌         | 13/243 [00:08<02:20,  1.64ba/s]Running tokenizer on every text in dataset:   6%|▌         | 14/243 [00:08<02:24,  1.59ba/s]Running tokenizer on every text in dataset:   6%|▌         | 15/243 [00:09<02:28,  1.53ba/s]Running tokenizer on every text in dataset:   7%|▋         | 16/243 [00:10<02:23,  1.58ba/s]Running tokenizer on every text in dataset:   7%|▋         | 17/243 [00:10<02:22,  1.58ba/s]Running tokenizer on every text in dataset:   7%|▋         | 18/243 [00:11<02:23,  1.57ba/s]Running tokenizer on every text in dataset:   8%|▊         | 19/243 [00:12<02:18,  1.62ba/s]Running tokenizer on every text in dataset:   8%|▊         | 20/243 [00:12<02:17,  1.63ba/s]Running tokenizer on every text in dataset:   9%|▊         | 21/243 [00:13<02:13,  1.66ba/s]Running tokenizer on every text in dataset:   9%|▉         | 22/243 [00:13<02:11,  1.69ba/s]Running tokenizer on every text in dataset:   9%|▉         | 23/243 [00:14<02:08,  1.72ba/s]Running tokenizer on every text in dataset:  10%|▉         | 24/243 [00:15<02:12,  1.65ba/s]Running tokenizer on every text in dataset:  10%|█         | 25/243 [00:15<02:13,  1.64ba/s]Running tokenizer on every text in dataset:  11%|█         | 26/243 [00:16<02:16,  1.59ba/s]Running tokenizer on every text in dataset:  11%|█         | 27/243 [00:17<02:21,  1.53ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 28/243 [00:17<02:14,  1.59ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 29/243 [00:18<02:13,  1.61ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 30/243 [00:18<02:16,  1.56ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 31/243 [00:19<02:14,  1.57ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 32/243 [00:20<02:14,  1.56ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 33/243 [00:20<02:11,  1.60ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 34/243 [00:21<02:11,  1.58ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 35/243 [00:22<02:06,  1.65ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 36/243 [00:22<02:04,  1.66ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 37/243 [00:23<02:10,  1.58ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 38/243 [00:23<02:02,  1.67ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 39/243 [00:24<02:01,  1.68ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 40/243 [00:24<01:57,  1.73ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 41/243 [00:25<01:58,  1.71ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 42/243 [00:26<01:59,  1.69ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 43/243 [00:26<02:13,  1.50ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 44/243 [00:27<02:10,  1.52ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 45/243 [00:28<02:08,  1.54ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 46/243 [00:28<02:02,  1.60ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 47/243 [00:29<02:01,  1.62ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 48/243 [00:30<02:05,  1.55ba/s]Running tokenizer on every text in dataset:  20%|██        | 49/243 [00:30<02:10,  1.49ba/s]Running tokenizer on every text in dataset:  21%|██        | 50/243 [00:31<02:12,  1.46ba/s]Running tokenizer on every text in dataset:  21%|██        | 51/243 [00:32<02:15,  1.42ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 52/243 [00:32<02:07,  1.50ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 53/243 [00:33<01:59,  1.59ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 54/243 [00:34<01:57,  1.60ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 55/243 [00:34<01:57,  1.60ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 56/243 [00:35<02:00,  1.56ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 57/243 [00:35<01:51,  1.67ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 58/243 [00:36<01:48,  1.71ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 59/243 [00:36<01:45,  1.75ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 60/243 [00:37<01:45,  1.74ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 61/243 [00:38<01:51,  1.63ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 62/243 [00:38<01:58,  1.53ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 63/243 [00:39<02:02,  1.47ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 64/243 [00:40<01:56,  1.54ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 65/243 [00:40<01:52,  1.58ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 66/243 [00:41<01:51,  1.59ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 67/243 [00:42<01:48,  1.63ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 68/243 [00:42<01:47,  1.63ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 69/243 [00:43<01:45,  1.65ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 70/243 [00:44<01:50,  1.57ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 71/243 [00:44<01:48,  1.58ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 72/243 [00:45<01:45,  1.62ba/s]Running tokenizer on every text in dataset:  30%|███       | 73/243 [00:45<01:45,  1.61ba/s]Running tokenizer on every text in dataset:  30%|███       | 74/243 [00:46<01:42,  1.64ba/s]Running tokenizer on every text in dataset:  31%|███       | 75/243 [00:46<01:37,  1.72ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 76/243 [00:47<01:36,  1.73ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 77/243 [00:48<01:37,  1.71ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 78/243 [00:48<01:43,  1.59ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 79/243 [00:49<01:36,  1.71ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 80/243 [00:49<01:33,  1.74ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 81/243 [00:50<01:34,  1.72ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 82/243 [00:51<01:47,  1.50ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 83/243 [00:52<01:45,  1.51ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 84/243 [00:52<01:40,  1.58ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 85/243 [00:53<01:38,  1.60ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 86/243 [00:53<01:37,  1.60ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 87/243 [00:54<01:41,  1.54ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 88/243 [00:55<01:39,  1.55ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 89/243 [00:55<01:40,  1.54ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 90/243 [00:56<01:38,  1.55ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 91/243 [00:57<01:40,  1.52ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 92/243 [00:57<01:38,  1.53ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 93/243 [00:58<01:35,  1.57ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 94/243 [00:58<01:33,  1.59ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 95/243 [00:59<01:31,  1.62ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 96/243 [01:00<01:28,  1.66ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 97/243 [01:00<01:34,  1.55ba/s]Running tokenizer on every text in dataset:  40%|████      | 98/243 [01:01<01:31,  1.59ba/s]Running tokenizer on every text in dataset:  41%|████      | 99/243 [01:02<01:31,  1.57ba/s]Running tokenizer on every text in dataset:  41%|████      | 100/243 [01:02<01:28,  1.61ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 101/243 [01:03<01:29,  1.59ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 102/243 [01:03<01:27,  1.61ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 103/243 [01:04<01:27,  1.60ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 104/243 [01:05<01:25,  1.62ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 105/243 [01:05<01:25,  1.62ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 106/243 [01:06<01:22,  1.65ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 107/243 [01:06<01:21,  1.66ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 108/243 [01:07<01:22,  1.64ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 109/243 [01:08<01:22,  1.63ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 110/243 [01:08<01:22,  1.62ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 111/243 [01:09<01:26,  1.52ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 112/243 [01:10<01:22,  1.58ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 113/243 [01:10<01:26,  1.51ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 114/243 [01:11<01:26,  1.48ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 115/243 [01:12<01:19,  1.61ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 116/243 [01:12<01:20,  1.59ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 117/243 [01:13<01:18,  1.61ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 118/243 [01:13<01:15,  1.67ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 119/243 [01:14<01:16,  1.61ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 120/243 [01:15<01:13,  1.67ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 121/243 [01:15<01:12,  1.68ba/s]Running tokenizer on every text in dataset:  50%|█████     | 122/243 [01:16<01:09,  1.75ba/s]Running tokenizer on every text in dataset:  51%|█████     | 123/243 [01:16<01:11,  1.67ba/s]Running tokenizer on every text in dataset:  51%|█████     | 124/243 [01:17<01:14,  1.59ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 125/243 [01:18<01:15,  1.57ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 126/243 [01:18<01:12,  1.61ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 127/243 [01:19<01:11,  1.63ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 128/243 [01:20<01:12,  1.58ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 129/243 [01:20<01:10,  1.62ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 130/243 [01:21<01:11,  1.58ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 131/243 [01:21<01:10,  1.58ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 132/243 [01:22<01:12,  1.52ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 133/243 [01:23<01:08,  1.61ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 134/243 [01:23<01:07,  1.61ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 135/243 [01:24<01:02,  1.74ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 136/243 [01:25<01:05,  1.64ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 137/243 [01:25<01:05,  1.62ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 138/243 [01:26<01:07,  1.55ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 139/243 [01:26<01:03,  1.63ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 140/243 [01:27<01:01,  1.67ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 141/243 [01:28<01:00,  1.68ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 142/243 [01:28<00:59,  1.69ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 143/243 [01:29<01:00,  1.65ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 144/243 [01:29<01:01,  1.62ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 145/243 [01:30<01:02,  1.56ba/s]Running tokenizer on every text in dataset:  60%|██████    | 146/243 [01:31<01:00,  1.59ba/s]Running tokenizer on every text in dataset:  60%|██████    | 147/243 [01:31<01:02,  1.54ba/s]Running tokenizer on every text in dataset:  61%|██████    | 148/243 [01:32<01:04,  1.48ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 149/243 [01:33<01:00,  1.55ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 150/243 [01:33<00:57,  1.60ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 151/243 [01:34<00:55,  1.66ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 152/243 [01:34<00:54,  1.67ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 153/243 [01:35<00:54,  1.64ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 154/243 [01:36<00:54,  1.64ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 155/243 [01:36<00:53,  1.63ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 156/243 [01:37<00:56,  1.54ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 157/243 [01:38<00:57,  1.51ba/s]Running tokenizer on every text in dataset:  65%|██████▌   | 158/243 [01:38<00:55,  1.54ba/s]Running tokenizer on every text in dataset:  65%|██████▌   | 159/243 [01:39<00:55,  1.52ba/s]Running tokenizer on every text in dataset:  66%|██████▌   | 160/243 [01:40<00:53,  1.54ba/s]Running tokenizer on every text in dataset:  66%|██████▋   | 161/243 [01:40<00:51,  1.60ba/s]Running tokenizer on every text in dataset:  67%|██████▋   | 162/243 [01:41<00:46,  1.75ba/s]Running tokenizer on every text in dataset:  67%|██████▋   | 163/243 [01:41<00:46,  1.72ba/s]Running tokenizer on every text in dataset:  67%|██████▋   | 164/243 [01:42<00:44,  1.78ba/s]Running tokenizer on every text in dataset:  68%|██████▊   | 165/243 [01:42<00:45,  1.72ba/s]Running tokenizer on every text in dataset:  68%|██████▊   | 166/243 [01:43<00:45,  1.69ba/s]Running tokenizer on every text in dataset:  69%|██████▊   | 167/243 [01:44<00:47,  1.59ba/s]Running tokenizer on every text in dataset:  69%|██████▉   | 168/243 [01:44<00:48,  1.54ba/s]Running tokenizer on every text in dataset:  70%|██████▉   | 169/243 [01:45<00:46,  1.58ba/s]Running tokenizer on every text in dataset:  70%|██████▉   | 170/243 [01:46<00:45,  1.62ba/s]Running tokenizer on every text in dataset:  70%|███████   | 171/243 [01:46<00:45,  1.57ba/s]Running tokenizer on every text in dataset:  71%|███████   | 172/243 [01:47<00:45,  1.56ba/s]Running tokenizer on every text in dataset:  71%|███████   | 173/243 [01:48<00:44,  1.56ba/s]Running tokenizer on every text in dataset:  72%|███████▏  | 174/243 [01:48<00:43,  1.58ba/s]Running tokenizer on every text in dataset:  72%|███████▏  | 175/243 [01:49<00:42,  1.61ba/s]Running tokenizer on every text in dataset:  72%|███████▏  | 176/243 [01:50<00:43,  1.55ba/s]Running tokenizer on every text in dataset:  73%|███████▎  | 177/243 [01:50<00:42,  1.55ba/s]Running tokenizer on every text in dataset:  73%|███████▎  | 178/243 [01:51<00:40,  1.62ba/s]Running tokenizer on every text in dataset:  74%|███████▎  | 179/243 [01:51<00:40,  1.56ba/s]Running tokenizer on every text in dataset:  74%|███████▍  | 180/243 [01:52<00:39,  1.59ba/s]Running tokenizer on every text in dataset:  74%|███████▍  | 181/243 [01:53<00:39,  1.56ba/s]Running tokenizer on every text in dataset:  75%|███████▍  | 182/243 [01:53<00:38,  1.60ba/s]Running tokenizer on every text in dataset:  75%|███████▌  | 183/243 [01:54<00:39,  1.51ba/s]Running tokenizer on every text in dataset:  76%|███████▌  | 184/243 [01:55<00:37,  1.57ba/s]Running tokenizer on every text in dataset:  76%|███████▌  | 185/243 [01:55<00:39,  1.48ba/s]Running tokenizer on every text in dataset:  77%|███████▋  | 186/243 [01:56<00:36,  1.55ba/s]Running tokenizer on every text in dataset:  77%|███████▋  | 187/243 [01:57<00:35,  1.59ba/s]Running tokenizer on every text in dataset:  77%|███████▋  | 188/243 [01:57<00:33,  1.64ba/s]Running tokenizer on every text in dataset:  78%|███████▊  | 189/243 [01:58<00:32,  1.64ba/s]Running tokenizer on every text in dataset:  78%|███████▊  | 190/243 [01:58<00:33,  1.60ba/s]Running tokenizer on every text in dataset:  79%|███████▊  | 191/243 [01:59<00:32,  1.62ba/s]Running tokenizer on every text in dataset:  79%|███████▉  | 192/243 [01:59<00:30,  1.69ba/s]Running tokenizer on every text in dataset:  79%|███████▉  | 193/243 [02:00<00:30,  1.66ba/s]Running tokenizer on every text in dataset:  80%|███████▉  | 194/243 [02:01<00:29,  1.66ba/s]Running tokenizer on every text in dataset:  80%|████████  | 195/243 [02:01<00:28,  1.68ba/s]Running tokenizer on every text in dataset:  81%|████████  | 196/243 [02:02<00:28,  1.65ba/s]Running tokenizer on every text in dataset:  81%|████████  | 197/243 [02:02<00:27,  1.69ba/s]Running tokenizer on every text in dataset:  81%|████████▏ | 198/243 [02:03<00:24,  1.81ba/s]Running tokenizer on every text in dataset:  82%|████████▏ | 199/243 [02:04<00:24,  1.78ba/s]Running tokenizer on every text in dataset:  82%|████████▏ | 200/243 [02:04<00:25,  1.71ba/s]Running tokenizer on every text in dataset:  83%|████████▎ | 201/243 [02:05<00:25,  1.64ba/s]Running tokenizer on every text in dataset:  83%|████████▎ | 202/243 [02:06<00:28,  1.46ba/s]Running tokenizer on every text in dataset:  84%|████████▎ | 203/243 [02:06<00:26,  1.53ba/s]Running tokenizer on every text in dataset:  84%|████████▍ | 204/243 [02:07<00:24,  1.62ba/s]Running tokenizer on every text in dataset:  84%|████████▍ | 205/243 [02:07<00:22,  1.66ba/s]Running tokenizer on every text in dataset:  85%|████████▍ | 206/243 [02:08<00:22,  1.65ba/s]Running tokenizer on every text in dataset:  85%|████████▌ | 207/243 [02:09<00:21,  1.64ba/s]Running tokenizer on every text in dataset:  86%|████████▌ | 208/243 [02:09<00:20,  1.72ba/s]Running tokenizer on every text in dataset:  86%|████████▌ | 209/243 [02:10<00:19,  1.71ba/s]Running tokenizer on every text in dataset:  86%|████████▋ | 210/243 [02:10<00:19,  1.68ba/s]Running tokenizer on every text in dataset:  87%|████████▋ | 211/243 [02:11<00:19,  1.65ba/s]Running tokenizer on every text in dataset:  87%|████████▋ | 212/243 [02:12<00:19,  1.63ba/s]Running tokenizer on every text in dataset:  88%|████████▊ | 213/243 [02:12<00:18,  1.62ba/s]Running tokenizer on every text in dataset:  88%|████████▊ | 214/243 [02:13<00:17,  1.62ba/s]Running tokenizer on every text in dataset:  88%|████████▊ | 215/243 [02:13<00:16,  1.66ba/s]Running tokenizer on every text in dataset:  89%|████████▉ | 216/243 [02:14<00:16,  1.63ba/s]Running tokenizer on every text in dataset:  89%|████████▉ | 217/243 [02:15<00:15,  1.64ba/s]Running tokenizer on every text in dataset:  90%|████████▉ | 218/243 [02:15<00:16,  1.54ba/s]Running tokenizer on every text in dataset:  90%|█████████ | 219/243 [02:16<00:15,  1.55ba/s]Running tokenizer on every text in dataset:  91%|█████████ | 220/243 [02:17<00:14,  1.58ba/s]Running tokenizer on every text in dataset:  91%|█████████ | 221/243 [02:17<00:13,  1.64ba/s]Running tokenizer on every text in dataset:  91%|█████████▏| 222/243 [02:18<00:12,  1.63ba/s]Running tokenizer on every text in dataset:  92%|█████████▏| 223/243 [02:19<00:13,  1.53ba/s]Running tokenizer on every text in dataset:  92%|█████████▏| 224/243 [02:19<00:11,  1.60ba/s]Running tokenizer on every text in dataset:  93%|█████████▎| 225/243 [02:20<00:10,  1.64ba/s]Running tokenizer on every text in dataset:  93%|█████████▎| 226/243 [02:20<00:10,  1.60ba/s]Running tokenizer on every text in dataset:  93%|█████████▎| 227/243 [02:21<00:09,  1.65ba/s]Running tokenizer on every text in dataset:  94%|█████████▍| 228/243 [02:22<00:08,  1.67ba/s]Running tokenizer on every text in dataset:  94%|█████████▍| 229/243 [02:22<00:08,  1.63ba/s]Running tokenizer on every text in dataset:  95%|█████████▍| 230/243 [02:23<00:08,  1.62ba/s]Running tokenizer on every text in dataset:  95%|█████████▌| 231/243 [02:23<00:07,  1.66ba/s]Running tokenizer on every text in dataset:  95%|█████████▌| 232/243 [02:24<00:06,  1.69ba/s]Running tokenizer on every text in dataset:  96%|█████████▌| 233/243 [02:25<00:05,  1.69ba/s]Running tokenizer on every text in dataset:  96%|█████████▋| 234/243 [02:25<00:05,  1.75ba/s]Running tokenizer on every text in dataset:  97%|█████████▋| 235/243 [02:26<00:04,  1.71ba/s]Running tokenizer on every text in dataset:  97%|█████████▋| 236/243 [02:26<00:04,  1.73ba/s]Running tokenizer on every text in dataset:  98%|█████████▊| 237/243 [02:27<00:03,  1.52ba/s]Running tokenizer on every text in dataset:  98%|█████████▊| 238/243 [02:28<00:03,  1.51ba/s]Running tokenizer on every text in dataset:  98%|█████████▊| 239/243 [02:28<00:02,  1.59ba/s]Running tokenizer on every text in dataset:  99%|█████████▉| 240/243 [02:29<00:01,  1.65ba/s]Running tokenizer on every text in dataset:  99%|█████████▉| 241/243 [02:29<00:01,  1.63ba/s]Running tokenizer on every text in dataset: 100%|█████████▉| 242/243 [02:30<00:00,  1.68ba/s]Running tokenizer on every text in dataset: 100%|██████████| 243/243 [02:30<00:00,  2.10ba/s]Running tokenizer on every text in dataset: 100%|██████████| 243/243 [02:30<00:00,  1.61ba/s]
Running tokenizer on every text in dataset:   0%|          | 0/13 [00:00<?, ?ba/s]Running tokenizer on every text in dataset:   8%|▊         | 1/13 [00:00<00:07,  1.62ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 2/13 [00:01<00:06,  1.67ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 3/13 [00:01<00:06,  1.65ba/s]Running tokenizer on every text in dataset:  31%|███       | 4/13 [00:02<00:05,  1.65ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 5/13 [00:02<00:04,  1.78ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 6/13 [00:03<00:03,  1.77ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 7/13 [00:04<00:03,  1.75ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 8/13 [00:04<00:02,  1.79ba/s]Running tokenizer on every text in dataset:  69%|██████▉   | 9/13 [00:05<00:02,  1.78ba/s]Running tokenizer on every text in dataset:  77%|███████▋  | 10/13 [00:05<00:01,  1.59ba/s]Running tokenizer on every text in dataset:  85%|████████▍ | 11/13 [00:06<00:01,  1.56ba/s]Running tokenizer on every text in dataset:  92%|█████████▏| 12/13 [00:07<00:00,  1.63ba/s]Running tokenizer on every text in dataset: 100%|██████████| 13/13 [00:07<00:00,  1.79ba/s]Running tokenizer on every text in dataset: 100%|██████████| 13/13 [00:07<00:00,  1.71ba/s]
Grouping texts in chunks of 1024:   0%|          | 0/243 [00:00<?, ?ba/s]Grouping texts in chunks of 1024:   0%|          | 1/243 [00:03<12:59,  3.22s/ba]Grouping texts in chunks of 1024:   1%|          | 2/243 [00:06<13:21,  3.33s/ba]Grouping texts in chunks of 1024:   1%|          | 3/243 [00:09<12:48,  3.20s/ba]Grouping texts in chunks of 1024:   2%|▏         | 4/243 [00:13<13:04,  3.28s/ba]Grouping texts in chunks of 1024:   2%|▏         | 5/243 [00:15<12:20,  3.11s/ba]Grouping texts in chunks of 1024:   2%|▏         | 6/243 [00:18<11:34,  2.93s/ba]Grouping texts in chunks of 1024:   3%|▎         | 7/243 [00:21<11:26,  2.91s/ba]Grouping texts in chunks of 1024:   3%|▎         | 8/243 [00:24<11:54,  3.04s/ba]Grouping texts in chunks of 1024:   4%|▎         | 9/243 [00:27<12:12,  3.13s/ba]Grouping texts in chunks of 1024:   4%|▍         | 10/243 [00:30<11:30,  2.96s/ba]Grouping texts in chunks of 1024:   5%|▍         | 11/243 [00:33<11:30,  2.98s/ba]Grouping texts in chunks of 1024:   5%|▍         | 12/243 [00:36<11:46,  3.06s/ba]Grouping texts in chunks of 1024:   5%|▌         | 13/243 [00:40<12:00,  3.13s/ba]Grouping texts in chunks of 1024:   6%|▌         | 14/243 [00:43<12:34,  3.30s/ba]Grouping texts in chunks of 1024:   6%|▌         | 15/243 [00:47<12:25,  3.27s/ba]Grouping texts in chunks of 1024:   7%|▋         | 16/243 [00:50<12:16,  3.24s/ba]Grouping texts in chunks of 1024:   7%|▋         | 17/243 [00:53<12:22,  3.28s/ba]Grouping texts in chunks of 1024:   7%|▋         | 18/243 [00:56<11:56,  3.19s/ba]Grouping texts in chunks of 1024:   8%|▊         | 19/243 [00:59<11:33,  3.10s/ba]Grouping texts in chunks of 1024:   8%|▊         | 20/243 [01:02<11:32,  3.10s/ba]Grouping texts in chunks of 1024:   9%|▊         | 21/243 [01:05<11:19,  3.06s/ba]Grouping texts in chunks of 1024:   9%|▉         | 22/243 [01:08<11:00,  2.99s/ba]Grouping texts in chunks of 1024:   9%|▉         | 23/243 [01:10<10:31,  2.87s/ba]Grouping texts in chunks of 1024:  10%|▉         | 24/243 [01:14<11:30,  3.15s/ba]Grouping texts in chunks of 1024:  10%|█         | 25/243 [01:17<11:34,  3.19s/ba]Grouping texts in chunks of 1024:  11%|█         | 26/243 [01:21<11:40,  3.23s/ba]Grouping texts in chunks of 1024:  11%|█         | 27/243 [01:24<11:06,  3.08s/ba]Grouping texts in chunks of 1024:  12%|█▏        | 28/243 [01:26<10:52,  3.03s/ba]Grouping texts in chunks of 1024:  12%|█▏        | 29/243 [01:30<10:56,  3.07s/ba]Grouping texts in chunks of 1024:  12%|█▏        | 30/243 [01:33<11:14,  3.16s/ba]Grouping texts in chunks of 1024:  13%|█▎        | 31/243 [01:36<11:16,  3.19s/ba]Grouping texts in chunks of 1024:  13%|█▎        | 32/243 [01:40<11:37,  3.30s/ba]Grouping texts in chunks of 1024:  14%|█▎        | 33/243 [01:43<11:18,  3.23s/ba]Grouping texts in chunks of 1024:  14%|█▍        | 34/243 [01:47<11:41,  3.36s/ba]Grouping texts in chunks of 1024:  14%|█▍        | 35/243 [01:49<11:06,  3.21s/ba]Grouping texts in chunks of 1024:  15%|█▍        | 36/243 [01:52<10:51,  3.15s/ba]Grouping texts in chunks of 1024:  15%|█▌        | 37/243 [01:56<11:33,  3.37s/ba]Grouping texts in chunks of 1024:  16%|█▌        | 38/243 [01:59<10:45,  3.15s/ba]Grouping texts in chunks of 1024:  16%|█▌        | 39/243 [02:02<10:20,  3.04s/ba]Grouping texts in chunks of 1024:  16%|█▋        | 40/243 [02:04<10:00,  2.96s/ba]Grouping texts in chunks of 1024:  17%|█▋        | 41/243 [02:08<10:10,  3.02s/ba]Grouping texts in chunks of 1024:  17%|█▋        | 42/243 [02:11<10:14,  3.06s/ba]Grouping texts in chunks of 1024:  18%|█▊        | 43/243 [02:14<10:35,  3.18s/ba]Grouping texts in chunks of 1024:  18%|█▊        | 44/243 [02:18<10:44,  3.24s/ba]Grouping texts in chunks of 1024:  19%|█▊        | 45/243 [02:21<10:38,  3.22s/ba]Grouping texts in chunks of 1024:  19%|█▉        | 46/243 [02:24<10:11,  3.10s/ba]Grouping texts in chunks of 1024:  19%|█▉        | 47/243 [02:27<10:09,  3.11s/ba]Grouping texts in chunks of 1024:  20%|█▉        | 48/243 [02:30<10:32,  3.24s/ba]Grouping texts in chunks of 1024:  20%|██        | 49/243 [02:34<11:15,  3.48s/ba]Grouping texts in chunks of 1024:  21%|██        | 50/243 [02:38<11:28,  3.57s/ba]Grouping texts in chunks of 1024:  21%|██        | 51/243 [02:42<11:41,  3.65s/ba]Grouping texts in chunks of 1024:  21%|██▏       | 52/243 [02:45<10:56,  3.44s/ba]Grouping texts in chunks of 1024:  22%|██▏       | 53/243 [02:48<10:11,  3.22s/ba]Grouping texts in chunks of 1024:  22%|██▏       | 54/243 [02:51<10:13,  3.24s/ba]Grouping texts in chunks of 1024:  23%|██▎       | 55/243 [02:54<10:23,  3.32s/ba]Grouping texts in chunks of 1024:  23%|██▎       | 56/243 [02:58<10:33,  3.39s/ba]Grouping texts in chunks of 1024:  23%|██▎       | 57/243 [03:01<09:52,  3.19s/ba]Grouping texts in chunks of 1024:  24%|██▍       | 58/243 [03:04<09:39,  3.13s/ba]Grouping texts in chunks of 1024:  24%|██▍       | 59/243 [03:07<09:24,  3.07s/ba]Grouping texts in chunks of 1024:  25%|██▍       | 60/243 [03:10<09:17,  3.05s/ba]Grouping texts in chunks of 1024:  25%|██▌       | 61/243 [03:13<09:45,  3.22s/ba]Grouping texts in chunks of 1024:  26%|██▌       | 62/243 [03:16<09:37,  3.19s/ba]Grouping texts in chunks of 1024:  26%|██▌       | 63/243 [03:20<10:14,  3.42s/ba]Grouping texts in chunks of 1024:  26%|██▋       | 64/243 [03:23<09:40,  3.24s/ba]Grouping texts in chunks of 1024:  27%|██▋       | 65/243 [03:26<09:33,  3.22s/ba]Grouping texts in chunks of 1024:  27%|██▋       | 66/243 [03:30<09:35,  3.25s/ba]Grouping texts in chunks of 1024:  28%|██▊       | 67/243 [03:32<09:11,  3.14s/ba]Grouping texts in chunks of 1024:  28%|██▊       | 68/243 [03:36<09:09,  3.14s/ba]Grouping texts in chunks of 1024:  28%|██▊       | 69/243 [03:39<08:59,  3.10s/ba]Grouping texts in chunks of 1024:  29%|██▉       | 70/243 [03:42<09:33,  3.32s/ba]Grouping texts in chunks of 1024:  29%|██▉       | 71/243 [03:46<09:31,  3.32s/ba]Grouping texts in chunks of 1024:  30%|██▉       | 72/243 [03:49<09:19,  3.27s/ba]Grouping texts in chunks of 1024:  30%|███       | 73/243 [03:52<09:21,  3.30s/ba]Grouping texts in chunks of 1024:  30%|███       | 74/243 [03:56<09:31,  3.38s/ba]Grouping texts in chunks of 1024:  31%|███       | 75/243 [03:59<08:58,  3.20s/ba]Grouping texts in chunks of 1024:  31%|███▏      | 76/243 [04:02<08:38,  3.11s/ba]Grouping texts in chunks of 1024:  32%|███▏      | 77/243 [04:05<08:40,  3.14s/ba]Grouping texts in chunks of 1024:  32%|███▏      | 78/243 [04:08<08:32,  3.10s/ba]Grouping texts in chunks of 1024:  33%|███▎      | 79/243 [04:10<07:59,  2.92s/ba]Grouping texts in chunks of 1024:  33%|███▎      | 80/243 [04:13<07:52,  2.90s/ba]Grouping texts in chunks of 1024:  33%|███▎      | 81/243 [04:16<08:10,  3.03s/ba]Grouping texts in chunks of 1024:  34%|███▎      | 82/243 [04:20<08:45,  3.26s/ba]Grouping texts in chunks of 1024:  34%|███▍      | 83/243 [04:24<08:52,  3.33s/ba]Grouping texts in chunks of 1024:  35%|███▍      | 84/243 [04:27<08:28,  3.20s/ba]Grouping texts in chunks of 1024:  35%|███▍      | 85/243 [04:30<08:38,  3.28s/ba]Grouping texts in chunks of 1024:  35%|███▌      | 86/243 [04:33<08:18,  3.18s/ba]Grouping texts in chunks of 1024:  36%|███▌      | 87/243 [04:37<08:52,  3.41s/ba]Grouping texts in chunks of 1024:  36%|███▌      | 88/243 [04:40<08:28,  3.28s/ba]Grouping texts in chunks of 1024:  37%|███▋      | 89/243 [04:44<08:45,  3.41s/ba]Grouping texts in chunks of 1024:  37%|███▋      | 90/243 [04:47<08:24,  3.30s/ba]Grouping texts in chunks of 1024:  37%|███▋      | 91/243 [04:51<08:55,  3.53s/ba]Grouping texts in chunks of 1024:  38%|███▊      | 92/243 [04:54<08:47,  3.50s/ba]Grouping texts in chunks of 1024:  38%|███▊      | 93/243 [04:58<08:42,  3.48s/ba]Grouping texts in chunks of 1024:  39%|███▊      | 94/243 [05:01<08:40,  3.49s/ba]Grouping texts in chunks of 1024:  39%|███▉      | 95/243 [05:04<08:22,  3.40s/ba]Grouping texts in chunks of 1024:  40%|███▉      | 96/243 [05:07<07:59,  3.26s/ba]Grouping texts in chunks of 1024:  40%|███▉      | 97/243 [05:10<07:41,  3.16s/ba]Grouping texts in chunks of 1024:  40%|████      | 98/243 [05:13<07:26,  3.08s/ba]Grouping texts in chunks of 1024:  41%|████      | 99/243 [05:16<07:24,  3.08s/ba]Grouping texts in chunks of 1024:  41%|████      | 100/243 [05:19<07:15,  3.04s/ba]Grouping texts in chunks of 1024:  42%|████▏     | 101/243 [05:22<07:09,  3.03s/ba]Grouping texts in chunks of 1024:  42%|████▏     | 102/243 [05:25<07:15,  3.09s/ba]Grouping texts in chunks of 1024:  42%|████▏     | 103/243 [05:29<07:20,  3.15s/ba]Grouping texts in chunks of 1024:  43%|████▎     | 104/243 [05:32<07:26,  3.21s/ba]Grouping texts in chunks of 1024:  43%|████▎     | 105/243 [05:35<07:26,  3.24s/ba]Grouping texts in chunks of 1024:  44%|████▎     | 106/243 [05:38<07:14,  3.17s/ba]Grouping texts in chunks of 1024:  44%|████▍     | 107/243 [05:41<07:08,  3.15s/ba]Grouping texts in chunks of 1024:  44%|████▍     | 108/243 [05:44<06:57,  3.09s/ba]Grouping texts in chunks of 1024:  45%|████▍     | 109/243 [05:48<07:10,  3.21s/ba]Grouping texts in chunks of 1024:  45%|████▌     | 110/243 [05:51<06:49,  3.08s/ba]Grouping texts in chunks of 1024:  46%|████▌     | 111/243 [05:54<07:07,  3.24s/ba]Grouping texts in chunks of 1024:  46%|████▌     | 112/243 [05:57<06:54,  3.16s/ba]Grouping texts in chunks of 1024:  47%|████▋     | 113/243 [06:01<07:00,  3.23s/ba]Grouping texts in chunks of 1024:  47%|████▋     | 114/243 [06:04<07:14,  3.37s/ba]Grouping texts in chunks of 1024:  47%|████▋     | 115/243 [06:07<06:42,  3.15s/ba]Grouping texts in chunks of 1024:  48%|████▊     | 116/243 [06:10<06:44,  3.18s/ba]Grouping texts in chunks of 1024:  48%|████▊     | 117/243 [06:13<06:42,  3.19s/ba]Grouping texts in chunks of 1024:  49%|████▊     | 118/243 [06:16<06:24,  3.08s/ba]Grouping texts in chunks of 1024:  49%|████▉     | 119/243 [06:20<06:33,  3.17s/ba]Grouping texts in chunks of 1024:  49%|████▉     | 120/243 [06:23<06:17,  3.07s/ba]Grouping texts in chunks of 1024:  50%|████▉     | 121/243 [06:25<06:08,  3.02s/ba]Grouping texts in chunks of 1024:  50%|█████     | 122/243 [06:28<05:54,  2.93s/ba]Grouping texts in chunks of 1024:  51%|█████     | 123/243 [06:31<06:05,  3.05s/ba]Grouping texts in chunks of 1024:  51%|█████     | 124/243 [06:35<06:26,  3.25s/ba]Grouping texts in chunks of 1024:  51%|█████▏    | 125/243 [06:39<06:41,  3.40s/ba]Grouping texts in chunks of 1024:  52%|█████▏    | 126/243 [06:42<06:18,  3.24s/ba]Grouping texts in chunks of 1024:  52%|█████▏    | 127/243 [06:45<06:12,  3.21s/ba]Grouping texts in chunks of 1024:  53%|█████▎    | 128/243 [06:48<06:19,  3.30s/ba]Grouping texts in chunks of 1024:  53%|█████▎    | 129/243 [06:52<06:09,  3.24s/ba]Grouping texts in chunks of 1024:  53%|█████▎    | 130/243 [06:55<06:04,  3.22s/ba]Grouping texts in chunks of 1024:  54%|█████▍    | 131/243 [06:58<05:57,  3.20s/ba]Grouping texts in chunks of 1024:  54%|█████▍    | 132/243 [07:01<05:56,  3.21s/ba]Grouping texts in chunks of 1024:  55%|█████▍    | 133/243 [07:04<05:46,  3.15s/ba]Grouping texts in chunks of 1024:  55%|█████▌    | 134/243 [07:07<05:34,  3.07s/ba]Grouping texts in chunks of 1024:  56%|█████▌    | 135/243 [07:10<05:16,  2.93s/ba]Grouping texts in chunks of 1024:  56%|█████▌    | 136/243 [07:13<05:27,  3.06s/ba]Grouping texts in chunks of 1024:  56%|█████▋    | 137/243 [07:16<05:26,  3.08s/ba]Grouping texts in chunks of 1024:  57%|█████▋    | 138/243 [07:19<05:26,  3.11s/ba]Grouping texts in chunks of 1024:  57%|█████▋    | 139/243 [07:22<05:18,  3.06s/ba]Grouping texts in chunks of 1024:  58%|█████▊    | 140/243 [07:25<05:11,  3.03s/ba]Grouping texts in chunks of 1024:  58%|█████▊    | 141/243 [07:28<05:13,  3.07s/ba]Grouping texts in chunks of 1024:  58%|█████▊    | 142/243 [07:31<05:02,  3.00s/ba]Grouping texts in chunks of 1024:  59%|█████▉    | 143/243 [07:35<05:11,  3.12s/ba]Grouping texts in chunks of 1024:  59%|█████▉    | 144/243 [07:38<05:23,  3.27s/ba]Grouping texts in chunks of 1024:  60%|█████▉    | 145/243 [07:42<05:26,  3.33s/ba]Grouping texts in chunks of 1024:  60%|██████    | 146/243 [07:45<05:24,  3.34s/ba]Grouping texts in chunks of 1024:  60%|██████    | 147/243 [07:49<05:35,  3.49s/ba]Grouping texts in chunks of 1024:  61%|██████    | 148/243 [07:52<05:13,  3.30s/ba]Grouping texts in chunks of 1024:  61%|██████▏   | 149/243 [07:55<05:08,  3.29s/ba]Grouping texts in chunks of 1024:  62%|██████▏   | 150/243 [07:58<05:04,  3.28s/ba]Grouping texts in chunks of 1024:  62%|██████▏   | 151/243 [08:01<04:48,  3.14s/ba]Grouping texts in chunks of 1024:  63%|██████▎   | 152/243 [08:04<04:47,  3.15s/ba]Grouping texts in chunks of 1024:  63%|██████▎   | 153/243 [08:08<04:49,  3.21s/ba]Grouping texts in chunks of 1024:  63%|██████▎   | 154/243 [08:11<04:49,  3.25s/ba]Grouping texts in chunks of 1024:  64%|██████▍   | 155/243 [08:14<04:49,  3.29s/ba]Grouping texts in chunks of 1024:  64%|██████▍   | 156/243 [08:18<05:00,  3.45s/ba]Grouping texts in chunks of 1024:  65%|██████▍   | 157/243 [08:22<05:16,  3.68s/ba]Grouping texts in chunks of 1024:  65%|██████▌   | 158/243 [08:25<04:57,  3.50s/ba]Grouping texts in chunks of 1024:  65%|██████▌   | 159/243 [08:29<04:48,  3.43s/ba]Grouping texts in chunks of 1024:  66%|██████▌   | 160/243 [08:32<04:37,  3.35s/ba]Grouping texts in chunks of 1024:  66%|██████▋   | 161/243 [08:35<04:30,  3.30s/ba]Grouping texts in chunks of 1024:  67%|██████▋   | 162/243 [08:37<04:05,  3.03s/ba]Grouping texts in chunks of 1024:  67%|██████▋   | 163/243 [08:41<04:07,  3.10s/ba]Grouping texts in chunks of 1024:  67%|██████▋   | 164/243 [08:43<03:53,  2.96s/ba]Grouping texts in chunks of 1024:  68%|██████▊   | 165/243 [08:47<04:01,  3.10s/ba]Grouping texts in chunks of 1024:  68%|██████▊   | 166/243 [08:50<04:07,  3.22s/ba]Grouping texts in chunks of 1024:  69%|██████▊   | 167/243 [08:53<04:01,  3.18s/ba]Grouping texts in chunks of 1024:  69%|██████▉   | 168/243 [08:57<04:13,  3.38s/ba]Grouping texts in chunks of 1024:  70%|██████▉   | 169/243 [09:00<04:00,  3.25s/ba]Grouping texts in chunks of 1024:  70%|██████▉   | 170/243 [09:03<03:52,  3.19s/ba]Grouping texts in chunks of 1024:  70%|███████   | 171/243 [09:07<03:52,  3.23s/ba]Grouping texts in chunks of 1024:  71%|███████   | 172/243 [09:10<03:47,  3.21s/ba]Grouping texts in chunks of 1024:  71%|███████   | 173/243 [09:13<03:46,  3.24s/ba]Grouping texts in chunks of 1024:  72%|███████▏  | 174/243 [09:16<03:44,  3.26s/ba]Grouping texts in chunks of 1024:  72%|███████▏  | 175/243 [09:19<03:37,  3.20s/ba]Grouping texts in chunks of 1024:  72%|███████▏  | 176/243 [09:23<03:41,  3.30s/ba]Grouping texts in chunks of 1024:  73%|███████▎  | 177/243 [09:26<03:41,  3.36s/ba]Grouping texts in chunks of 1024:  73%|███████▎  | 178/243 [09:29<03:27,  3.19s/ba]Grouping texts in chunks of 1024:  74%|███████▎  | 179/243 [09:33<03:29,  3.27s/ba]Grouping texts in chunks of 1024:  74%|███████▍  | 180/243 [09:36<03:27,  3.29s/ba]Grouping texts in chunks of 1024:  74%|███████▍  | 181/243 [09:39<03:24,  3.29s/ba]Grouping texts in chunks of 1024:  75%|███████▍  | 182/243 [09:42<03:15,  3.20s/ba]Grouping texts in chunks of 1024:  75%|███████▌  | 183/243 [09:46<03:21,  3.36s/ba]Grouping texts in chunks of 1024:  76%|███████▌  | 184/243 [09:49<03:13,  3.27s/ba]Grouping texts in chunks of 1024:  76%|███████▌  | 185/243 [09:53<03:22,  3.49s/ba]Grouping texts in chunks of 1024:  77%|███████▋  | 186/243 [09:56<03:11,  3.36s/ba]Grouping texts in chunks of 1024:  77%|███████▋  | 187/243 [10:00<03:10,  3.39s/ba]Grouping texts in chunks of 1024:  77%|███████▋  | 188/243 [10:03<03:01,  3.29s/ba]Grouping texts in chunks of 1024:  78%|███████▊  | 189/243 [10:06<02:57,  3.29s/ba]Grouping texts in chunks of 1024:  78%|███████▊  | 190/243 [10:10<03:02,  3.44s/ba]Grouping texts in chunks of 1024:  79%|███████▊  | 191/243 [10:13<02:57,  3.41s/ba]Grouping texts in chunks of 1024:  79%|███████▉  | 192/243 [10:16<02:45,  3.25s/ba]Grouping texts in chunks of 1024:  79%|███████▉  | 193/243 [10:19<02:44,  3.29s/ba]Grouping texts in chunks of 1024:  80%|███████▉  | 194/243 [10:22<02:34,  3.16s/ba]Grouping texts in chunks of 1024:  80%|████████  | 195/243 [10:25<02:31,  3.16s/ba]Grouping texts in chunks of 1024:  81%|████████  | 196/243 [10:29<02:29,  3.19s/ba]Grouping texts in chunks of 1024:  81%|████████  | 197/243 [10:32<02:23,  3.11s/ba]Grouping texts in chunks of 1024:  81%|████████▏ | 198/243 [10:34<02:12,  2.95s/ba]Grouping texts in chunks of 1024:  82%|████████▏ | 199/243 [10:37<02:14,  3.06s/ba]Grouping texts in chunks of 1024:  82%|████████▏ | 200/243 [10:41<02:17,  3.19s/ba]Grouping texts in chunks of 1024:  83%|████████▎ | 201/243 [10:45<02:20,  3.34s/ba]Grouping texts in chunks of 1024:  83%|████████▎ | 202/243 [10:48<02:18,  3.38s/ba]Grouping texts in chunks of 1024:  84%|████████▎ | 203/243 [10:51<02:12,  3.31s/ba]Grouping texts in chunks of 1024:  84%|████████▍ | 204/243 [10:54<02:05,  3.22s/ba]Grouping texts in chunks of 1024:  84%|████████▍ | 205/243 [10:57<02:01,  3.20s/ba]Grouping texts in chunks of 1024:  85%|████████▍ | 206/243 [11:00<01:55,  3.13s/ba]Grouping texts in chunks of 1024:  85%|████████▌ | 207/243 [11:04<01:55,  3.20s/ba]Grouping texts in chunks of 1024:  86%|████████▌ | 208/243 [11:06<01:45,  3.02s/ba]Grouping texts in chunks of 1024:  86%|████████▌ | 209/243 [11:09<01:43,  3.05s/ba]Grouping texts in chunks of 1024:  86%|████████▋ | 210/243 [11:13<01:43,  3.13s/ba]Grouping texts in chunks of 1024:  87%|████████▋ | 211/243 [11:16<01:41,  3.19s/ba]Grouping texts in chunks of 1024:  87%|████████▋ | 212/243 [11:19<01:39,  3.22s/ba]Grouping texts in chunks of 1024:  88%|████████▊ | 213/243 [11:23<01:38,  3.29s/ba]Grouping texts in chunks of 1024:  88%|████████▊ | 214/243 [11:26<01:36,  3.32s/ba]Grouping texts in chunks of 1024:  88%|████████▊ | 215/243 [11:29<01:30,  3.22s/ba]Grouping texts in chunks of 1024:  89%|████████▉ | 216/243 [11:33<01:28,  3.27s/ba]Grouping texts in chunks of 1024:  89%|████████▉ | 217/243 [11:35<01:22,  3.16s/ba]Grouping texts in chunks of 1024:  90%|████████▉ | 218/243 [11:39<01:20,  3.22s/ba]Grouping texts in chunks of 1024:  90%|█████████ | 219/243 [11:42<01:17,  3.23s/ba]Grouping texts in chunks of 1024:  91%|█████████ | 220/243 [11:45<01:15,  3.28s/ba]Grouping texts in chunks of 1024:  91%|█████████ | 221/243 [11:48<01:10,  3.18s/ba]Grouping texts in chunks of 1024:  91%|█████████▏| 222/243 [11:51<01:05,  3.14s/ba]Grouping texts in chunks of 1024:  92%|█████████▏| 223/243 [11:55<01:07,  3.36s/ba]Grouping texts in chunks of 1024:  92%|█████████▏| 224/243 [11:58<01:01,  3.22s/ba]Grouping texts in chunks of 1024:  93%|█████████▎| 225/243 [12:01<00:56,  3.14s/ba]Grouping texts in chunks of 1024:  93%|█████████▎| 226/243 [12:05<00:56,  3.30s/ba]Grouping texts in chunks of 1024:  93%|█████████▎| 227/243 [12:08<00:51,  3.23s/ba]Grouping texts in chunks of 1024:  94%|█████████▍| 228/243 [12:11<00:48,  3.24s/ba]Grouping texts in chunks of 1024:  94%|█████████▍| 229/243 [12:15<00:47,  3.37s/ba]Grouping texts in chunks of 1024:  95%|█████████▍| 230/243 [12:18<00:41,  3.21s/ba]Grouping texts in chunks of 1024:  95%|█████████▌| 231/243 [12:20<00:36,  3.06s/ba]Grouping texts in chunks of 1024:  95%|█████████▌| 232/243 [12:24<00:34,  3.10s/ba]Grouping texts in chunks of 1024:  96%|█████████▌| 233/243 [12:27<00:30,  3.08s/ba]Grouping texts in chunks of 1024:  96%|█████████▋| 234/243 [12:29<00:26,  2.99s/ba]Grouping texts in chunks of 1024:  97%|█████████▋| 235/243 [12:33<00:25,  3.22s/ba]Grouping texts in chunks of 1024:  97%|█████████▋| 236/243 [12:36<00:22,  3.19s/ba]Grouping texts in chunks of 1024:  98%|█████████▊| 237/243 [12:40<00:20,  3.34s/ba]Grouping texts in chunks of 1024:  98%|█████████▊| 238/243 [12:44<00:17,  3.43s/ba]Grouping texts in chunks of 1024:  98%|█████████▊| 239/243 [12:47<00:13,  3.26s/ba]Grouping texts in chunks of 1024:  99%|█████████▉| 240/243 [12:49<00:09,  3.15s/ba]Grouping texts in chunks of 1024:  99%|█████████▉| 241/243 [12:52<00:06,  3.08s/ba]Grouping texts in chunks of 1024: 100%|█████████▉| 242/243 [12:55<00:03,  3.07s/ba]Grouping texts in chunks of 1024: 100%|██████████| 243/243 [12:56<00:00,  2.25s/ba]Grouping texts in chunks of 1024: 100%|██████████| 243/243 [12:56<00:00,  3.19s/ba]
Grouping texts in chunks of 1024:   0%|          | 0/13 [00:00<?, ?ba/s]Grouping texts in chunks of 1024:   8%|▊         | 1/13 [00:03<00:36,  3.06s/ba]Grouping texts in chunks of 1024:  15%|█▌        | 2/13 [00:05<00:32,  2.93s/ba]Grouping texts in chunks of 1024:  23%|██▎       | 3/13 [00:09<00:31,  3.15s/ba]Grouping texts in chunks of 1024:  31%|███       | 4/13 [00:12<00:29,  3.22s/ba]Grouping texts in chunks of 1024:  38%|███▊      | 5/13 [00:15<00:24,  3.02s/ba]Grouping texts in chunks of 1024:  46%|████▌     | 6/13 [00:18<00:21,  3.05s/ba]Grouping texts in chunks of 1024:  54%|█████▍    | 7/13 [00:21<00:18,  3.11s/ba]Grouping texts in chunks of 1024:  62%|██████▏   | 8/13 [00:24<00:15,  3.02s/ba]Grouping texts in chunks of 1024:  69%|██████▉   | 9/13 [00:27<00:12,  3.15s/ba]Grouping texts in chunks of 1024:  77%|███████▋  | 10/13 [00:31<00:09,  3.20s/ba]Grouping texts in chunks of 1024:  85%|████████▍ | 11/13 [00:34<00:06,  3.35s/ba]Grouping texts in chunks of 1024:  92%|█████████▏| 12/13 [00:37<00:03,  3.25s/ba]Grouping texts in chunks of 1024: 100%|██████████| 13/13 [00:39<00:00,  2.84s/ba]Grouping texts in chunks of 1024: 100%|██████████| 13/13 [00:39<00:00,  3.06s/ba]
[INFO|trainer.py:434] 2021-09-24 00:41:26,680 >> Using amp fp16 backend
[INFO|trainer.py:540] 2021-09-24 00:41:26,690 >> The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.
[INFO|trainer.py:2230] 2021-09-24 00:41:26,802 >> ***** Running Evaluation *****
[INFO|trainer.py:2232] 2021-09-24 00:41:26,802 >>   Num examples = 6462
[INFO|trainer.py:2235] 2021-09-24 00:41:26,802 >>   Batch size = 8
Traceback (most recent call last):
  File "run_mlm.py", line 577, in <module>
    main()
  File "run_mlm.py", line 543, in main
    metrics = trainer.evaluate()
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py", line 2100, in evaluate
    output = eval_loop(
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py", line 2272, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py", line 2476, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/trainer.py", line 1870, in compute_loss
    outputs = model(**inputs)
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1090, in forward
    outputs = self.roberta(
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/gpfsnyu/home/yw3642/.conda/envs/kaggle/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 814, in forward
    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)
RuntimeError: The expanded size of the tensor (1024) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [8, 1024].  Tensor sizes: [1, 514]
